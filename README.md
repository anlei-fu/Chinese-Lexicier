# Chinese-Lexicier
Segmente Chinese Sentence  To Words
##### 字典保存在字符搜索树的结构中，
##### 字典的加载速度较慢，大概需要7,8秒
##### 内存使用量大概为70-80M
##### 如果想要提高字典加载速度，应重新设计字符搜索树的添加节点的模块
##### 消岐效果良好，分词速度大概20万字符每秒。
##### 如果需要提高分词速度，削减_WordInfo不必要属性并取消copy的方式，修改为传递引用

## 算法
###### 基于字符串扫描的方式，每当解析完成一个子句，会进行检测,出现单个字符，且这个字符不是特殊的单字词(如：和，与，我)等，确定为“有问题的”的序列，
###### 然后进行反向分词，比较正反向分词的结果
#### 比较的策略
######  总多字词个数最多,
######  单字词个数最小,
######  特殊单字（在，不，和等）词个数最多，
######  单字构词规则“较为”正确的结果。

## Usage
### eg
``` c#
 Lexicer lex = new Lexicer(new LexConfig("dic9.txt", "", "singleword2.txt", true, false));
var b=lex.Work("已经结婚的和尚未结婚的人，都要努力。");
/*********************************
 *返回一个passage对象赋值给b
  ***************************************/
```
